import os
import csv
import time
from pydub import AudioSegment
from collections import defaultdict
import traceback
import numpy as np
from pathlib import Path
from datetime import datetime
import config
from action.action_matcher import *
from audio.asr import PaddleSpeechRecognition, SpeechRecognitionAdapter
from audio.vector import PaddleSpeakerVerification, SpeakerVerificationAdapter
from config import AUDIO_TABLE, USER_TABLE
from dao.milvus_dao import MilvusClient
from dao.mysql_dao import MySQLClient
from utils.audioU import pre_process

ACCURACY_THRESHOLD = config.Algorithm.threshold
MODELS_DIR = config.Update.ModelDir

milvus_client = MilvusClient(config.Milvus.host, config.Milvus.port)
milvus_client.create_collection(AUDIO_TABLE)
mysql_client = MySQLClient(config.MySQL.host, config.MySQL.port, config.MySQL.user,
                           config.MySQL.password, config.MySQL.database)

paddleASR = SpeechRecognitionAdapter(PaddleSpeechRecognition())
paddleVector = SpeakerVerificationAdapter(PaddleSpeakerVerification())

action_matcher = InstructionMatcher(MODELS_DIR).load(MicomlMatcher('paraphrase-multilingual-MiniLM-L12-v2'))

def concatenate_audio_files(file_list):
    """
    å°†éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨æŒ‰é¡ºåºæ‹¼æ¥æˆæ–°æ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„
    å‚æ•°ï¼š
        file_list: è¦æ‹¼æ¥çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    è¿”å›ï¼š
        æˆåŠŸæ—¶è¿”å›æ–°æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    # æœ‰æ•ˆæ€§æ£€æŸ¥
    if not file_list:
        print("ğŸš« æ–‡ä»¶åˆ—è¡¨ä¸ºç©º")
        return None

    # åˆ›å»ºç©ºéŸ³é¢‘å®¹å™¨
    combined = AudioSegment.empty()
    valid_count = 0

    try:
        # éå†å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for idx, file_path in enumerate(file_list):
            try:
                # åŠ è½½éŸ³é¢‘æ–‡ä»¶
                audio = AudioSegment.from_file(file_path)
                combined += audio
                valid_count += 1

                # è®°å½•ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ–‡ä»¶çš„ç›®å½•ä¿¡æ¯
                if valid_count == 1:
                    first_file_dir = os.path.dirname(file_path)
                    first_file_name = os.path.basename(file_path)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–‡ä»¶ [{idx+1}/{len(file_list)}]: {file_path}")
                print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")

        # æœ‰æ•ˆæ€§æ£€æŸ¥
        if valid_count == 0:
            print("ğŸš« æ²¡æœ‰æœ‰æ•ˆéŸ³é¢‘æ–‡ä»¶å¯æ‹¼æ¥")
            return None

        # ç”Ÿæˆæ™ºèƒ½æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"combined_{timestamp}"

        # ä¿æŒä¸ç¬¬ä¸€ä¸ªæ–‡ä»¶ç›¸åŒçš„æ ¼å¼
        ext = os.path.splitext(first_file_name)[-1].lower()
        output_file = os.path.join(first_file_dir, f"{base_name}{ext}")

        # é˜²é‡å¤å¤„ç†
        counter = 1
        while os.path.exists(output_file):
            output_file = os.path.join(first_file_dir, f"{base_name}_{counter}{ext}")
            counter += 1

        # å¯¼å‡ºéŸ³é¢‘æ–‡ä»¶
        combined.export(output_file, format=ext[1:])  # å»é™¤æ‰©å±•åå‰çš„ç‚¹

        # æ‰“å°å¤„ç†æŠ¥å‘Š
        print(f"âœ… æˆåŠŸæ‹¼æ¥ {valid_count}/{len(file_list)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"â±ï¸ æ€»æ—¶é•¿: {len(combined)/1000:.2f}ç§’")

        return output_file

    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯: {str(e)}")
        return None


def batch_load(csv_path: str):
    try:
        start_warmup = time.perf_counter()  # æ–°å¢å¼€å§‹è®¡æ—¶
        mysql_client.get_all_users()  # åŸæœ‰é¢„çƒ­æ“ä½œ
    except Exception as e:
        warmup_duration = time.perf_counter() - start_warmup  # è®¡ç®—è€—æ—¶
        print(f"æ•°æ®åº“é¢„çƒ­å¤±è´¥ï¼ˆä¸å½±å“åç»­æ“ä½œï¼‰[è€—æ—¶ {warmup_duration:.2f}s]: {str(e)}")  # æ–°å¢è€—æ—¶æ˜¾ç¤º
        traceback.print_exc()
    else:
        warmup_duration = time.perf_counter() - start_warmup  # æˆåŠŸè€—æ—¶è®¡ç®—
        print(f"æ•°æ®åº“é¢„çƒ­æˆåŠŸ [è€—æ—¶ {warmup_duration:.2f}s]")  # æ–°å¢æˆåŠŸè€—æ—¶
    """æ‰¹é‡å¤„ç†CSVæ–‡ä»¶ä¸­çš„å£°çº¹æ³¨å†Œè¯·æ±‚"""
     # åˆ›å»ºç»“æœç›®å½•
    output_dir = Path("batch_registration_results")
    output_dir.mkdir(exist_ok=True)

    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
    success_file = output_dir / f"success_{timestamp}.csv"
    fail_file = output_dir / f"fail_{timestamp}.csv"
    time_log_file = output_dir / f"timing_{timestamp}.csv"

    with open(csv_path, 'r') as input_f, \
         open(success_file, 'w', newline='') as success_f, \
         open(fail_file, 'w', newline='') as fail_f, \
         open(time_log_file, 'w', newline='') as time_f:

        # åˆå§‹åŒ–å¢å¼ºç‰ˆå†™å…¥å™¨
        success_writer = csv.DictWriter(success_f, fieldnames=[
            'directory', 'file_count', 'username', 'permission_level',
            'user_id', 'voiceprint_id', 'processed_files'
        ])

        fail_writer = csv.DictWriter(fail_f, fieldnames=[
            'directory', 'file_count', 'username', 'error_type',
            'error_msg', 'failed_files'
        ])

        time_writer = csv.DictWriter(time_f, fieldnames=[
            'directory', 'preprocess', 'feature_extract', 'vector_avg',
            'milvus_insert', 'mysql_insert', 'total'
        ])

        # å†™å…¥è¡¨å¤´
        success_writer.writeheader()
        fail_writer.writeheader()
        time_writer.writeheader()

        # æ„å»ºç›®å½•åˆ†ç»„
        reader = csv.DictReader(input_f)
        dir_groups = defaultdict(list)
        for row in reader:
            dir_path = Path(row['filename']).parent
            dir_groups[dir_path].append(row)

        # å¤„ç†æ¯ä¸ªç›®å½•ç»„
        for dir_idx, (dir_path, rows) in enumerate(dir_groups.items(), 1):
            print(f"\nğŸ“‚ æ­£åœ¨å¤„ç†ç›®å½• {dir_idx}/{len(dir_groups)}: {dir_path}")

            # å‡†å¤‡åŸºç¡€æ•°æ®
            file_list = [row['filename'] for row in rows]
            username = rows[0]['username']
            permission_level = int(rows[0]['permission_level'])
            file_count = len(rows)

            # åˆå§‹åŒ–è®¡æ—¶
            timing = {'directory': str(dir_path)}
            start_total = time.perf_counter()

            try:
                # éªŒè¯æ•°æ®ä¸€è‡´æ€§
                if any(row['username'] != username for row in rows):
                    raise ValueError("ç›®å½•å†…å­˜åœ¨ä¸ä¸€è‡´çš„ç”¨æˆ·å")
                if any(int(row['permission_level']) != permission_level for row in rows):
                    raise ValueError("ç›®å½•å†…å­˜åœ¨ä¸ä¸€è‡´çš„æƒé™ç­‰çº§")

                # æ‰§è¡Œæ‰¹é‡æ³¨å†Œ
                result, timing = process_registration(
                    file_list=file_list,
                    username=username,
                    permission_level=permission_level,
                    timing=timing
                )

                # å†™å…¥æˆåŠŸè®°å½•
                success_writer.writerow({
                    'directory': str(dir_path),
                    'file_count': file_count,
                    'username': username,
                    'permission_level': permission_level,
                    'user_id': result['user_id'],
                    'voiceprint_id': result['voiceprint_id'],
                    'processed_files': ';'.join(file_list)
                })

            except Exception as e:
                # å†™å…¥å¤±è´¥è®°å½•
                fail_writer.writerow({
                    'directory': str(dir_path),
                    'file_count': file_count,
                    'username': username,
                    'error_type': type(e).__name__,
                    'error_msg': str(e),
                    'failed_files': ';'.join(file_list)
                })
                traceback.print_exc()
            finally:
                # è®°å½•æ—¶é—´æ—¥å¿—
                timing['total'] = time.perf_counter() - start_total
                time_writer.writerow(timing)

                # å®æ—¶åˆ·æ–°å†™å…¥
                success_f.flush()
                fail_f.flush()
                time_f.flush()

            time.sleep(1)  # æ¯ä¸ªæ–‡ä»¶å¤„ç†å®Œæš‚åœ

    print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")

def process_registration(file_list , username: str, permission_level: int, timing: dict) -> tuple:
    """å¤„ç†å•ä¸ªæ³¨å†Œè¯·æ±‚"""
    result = {}
    temp_files = []

    try:
        # === æ–‡ä»¶é¢„å¤„ç† ===
        start = time.perf_counter()
        processed_path = concatenate_audio_files(file_list)
        processed_path = pre_process(processed_path)
        temp_files.append(processed_path)
        timing['preprocess'] = time.perf_counter() - start

        # === ç‰¹å¾æå– ===
        start = time.perf_counter()
        audio_emb = paddleVector.get_embedding(processed_path)
        timing['feature_extract'] = time.perf_counter() - start

        # === å¤šæ–‡ä»¶å‘é‡å¹³å‡ ===
        start = time.perf_counter()
        avg_embedding = np.mean([audio_emb], axis=0)
        timing['vector_avg'] = time.perf_counter() - start

        # === Milvus æ“ä½œ ===
        start = time.perf_counter()
        milvus_ids = milvus_client.insert(AUDIO_TABLE, [avg_embedding.tolist()])
        milvus_client.create_index(AUDIO_TABLE)
        timing['milvus_insert'] = time.perf_counter() - start

        # === MySQL æ“ä½œ ===
        start = time.perf_counter()
        user_id = mysql_client.load_data_to_mysql(
            USER_TABLE,
            [(username, milvus_ids[0], permission_level)]
        )
        timing['mysql_insert'] = time.perf_counter() - start

        # ç»„è£…ç»“æœ
        result = {
            'user_id': user_id,
            'voiceprint_id': milvus_ids[0]
        }

    except Exception as e:
        raise RegistrationError(f"å¤„ç†å¤±è´¥: {str(e)}") from e

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for f in temp_files:
            if os.path.exists(f):
                try:
                    os.remove(f)
                except Exception as e:
                    print(f"æ¸…ç†æ–‡ä»¶å¤±è´¥: {str(e)}")

    return result, timing

class RegistrationError(Exception):
    """è‡ªå®šä¹‰æ³¨å†Œå¼‚å¸¸"""
    pass

if __name__ == "__main__":
    batch_load(r"P:\xiangmu\python\Voice\Data\Train_003.csv")
